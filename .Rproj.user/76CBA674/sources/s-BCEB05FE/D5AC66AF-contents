---
title: "Modeling, Testing, and Predicting: Health Related Data from Four Low-Income Communities in the Valley of Atlixco, Mexico"
date: 2020-03-16
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy.opts=list(width.cutoff=60),tidy=TRUE)

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}
```
## 0.Introduction
The data from 'healthmx' was collected during Summer 2019 as part of UT Austin's President's Award for Global Learning. Three rural pueblos and one urban colonia popular surrounding Atlixco, Puebla, Mexico were selected to conduct a comprehensive community health needs assessment in order to understand and address health inequities that are present in these low-income communities.The dataset used for this assignment was created from the semi-quantitative household survey data, is heavily redacted, and much smaller than the actual dataset, which had over 200 questions. This research was approved by the IRB and was  conducted under IRB guidelines. The data analysis for this project is exploratory and will not be used for publication or policy decisions. The 'number' variable relates to the relative timing of the survey, where 1 was the first household surveyed, and the variable 'rID' is the unique ID for the survey. The 'community' variable is the community where the survey was recorded and the 'housholdstucture' variable is the classification of the household such as nuclear or extended. The variable 'hp_visit' is the number of times someone in the houshold visited the household in the last three months from when the survey was taken. The 'transportation' variable is the type of transportation used to get to healthcare appointment and 'hp_time' is the amount of time it took to get to the healthcare appointment. The variable 'hp_payment' is the general amount paid for the healthcare services, where the options are none, part, or all. The 'payment_difficulty' is how difficult it was to absorb the costs related to healthcare in the household and  'confidence' was the amount of confidence in the provider the houshold had. The 'gender' variable is corresponds to the identified gender of the respondent, and the 'urban' variable was whether or not the community was classified as rural or urban. 'Hhsize' refers to the number of people living in the house and 'age' is the age of the respondent. The 'public_hp' is a variable to describe the primary healthcare utilization type of the household, where 1 is a public provider and 0 is private healthcare provider Overall, there are 242 observations or surveys in this dataset.


```{r}
library(dplyr); library(ggplot2); library(tidyverse); library(lmtest);
library(sandwich); library(plotROC);library(readr); library(pROC)
healthmx=read.csv('healthmx.csv')
glimpse(healthmx)
healthmx$female <-NULL
```

## 1. MANOVA and ANOVA Testing

```{r}
#MANOVA
healthmx1<-healthmx%>%drop_na(hhsize, age, hp_time, hp_payment)
man<-manova(cbind(hhsize,age,hp_time)~hp_payment, data=healthmx1)
summary(man)
#Homogeneity of (CO)Variances Assumption
covmats<-healthmx1%>%group_by(hp_payment)%>%do(covs=cov(.[13:15]))
for(i in 1:3){print(as.character(covmats$hp_payment[i])); print(covmats$covs[i])}
#ANOVA
summary(aov(hhsize~hp_payment,data=healthmx1))
summary(aov(age~hp_payment,data=healthmx1))
summary(aov(hp_time~hp_payment,data=healthmx1))
healthmx %>% count(hp_payment)
healthmx %>% group_by(hp_payment) %>% summarize(mean=sd(age, na.rm = TRUE))
healthmx %>% group_by(hp_payment) %>% summarize(mean=sd(hhsize, na.rm = TRUE))
healthmx %>% group_by(hp_payment) %>% summarize(mean=sd(hp_time, na.rm = TRUE))

#Post- Hoc T Tests 
pairwise.t.test(healthmx1$hhsize,healthmx1$hp_payment, p.adj = "none")
pairwise.t.test(healthmx1$age,healthmx1$hp_payment, p.adj = "none")
pairwise.t.test(healthmx1$hp_time,healthmx1$hp_payment, p.adj = "none")

#Probability of at least one Type 1 Error  
type1 <- 1-((1-0.05)^13)
type1

#Bonferroni Correction
bonferroni<-0.05/13
bonferroni
```
When performing a MANOVA test, we can see that there is a mean difference in healthcare payment level across household size, age, and time spent to get to healthcare services. Based on the significant p-value of 0.00975 of the MANOVA, there is a mean difference across healthcare payment levels with size, age, and time. With the MANOVA we are unsure as to which numeric variables show the difference, and therefore an ANOVA was performed. As for the assumptions of the MANOVA, the data did include random samples and independent observations, the multivariate normality of DV assumption was met because there were more than 25 samples in each payment group, and there appears to be linear relationship among DVs. However, there appears to be a lack of homogeneity of (co)variances according to the matrices, and there appears to be some univariate or multivariate outliers 

When performing the three ANOVA tests, it was found the the mean difference in healthcare payment occurs for the response variables of both age of respondent and amount of time it takes to get to healthcare provider. For an ANOVA, the assumptions include random sample and independent observations, independent samples, normal distribution in each group or a large sample, and equal variance of each group. These likely have been met because the respondents were randomly selected, the sampling of each respondent was done independently, there was over 25 samples for each payment group, and the standard deviation is not more than 2x bigger in one group versus another, except for the case of 'hp_time'.

Using the pairwise.t.test function, it was found that there is a difference in 'No, nada' and 'Todo' when age is used in the pairwise.t.test. It was also found that there is a difference between 'Todo' and 'Una parte' when time to get to the healthcare provider was used.

There was 1 MANOVA, 3 ANOVAs, and 9 pairwise t-tests that were performed. The probability of at least one type I error is  0.4866579 if unadjusted. The boneferroni adjusted significance level that should be used is to keep the overall type I error rate at 0.05 is 0.003846154. After using this significance level, we cannot state that any of the payment groups vary by 'hp_time','age', or 'hhsize'.


## 2. Randomization Test 

```{r}
#DropNA
healthmx_nona<-healthmx%>% drop_na(gender)%>% drop_na(age)
#Mean Difference 
healthmx_nona%>%group_by(gender)%>%summarize(means=mean(age))%>%summarize(`mean_diff:`=diff(means))

#Randomization Test
set.seed(34)
rand_dist<-vector()
for(i in 1:5000){
new<-data.frame(age=sample(healthmx_nona$age), gender=healthmx_nona$gender)
rand_dist[i]<-mean(new[new$gender=="F",]$age)-
  mean(new[new$gender=="M",]$age)}
mean(rand_dist>7.413414|rand_dist< -7.413414)

#Plot of Null Distribution and Test Statistic 
{hist(rand_dist,main="",ylab="Count (n)"); abline(v = -7.413414, col="red");abline(v = 7.413414, col="red")}
#Comparison to t-test
t.test(data=healthmx_nona, age~gender)
```
The null hypothesis of the randomization test is that the mean age is the same between male and female respondents. The alternative hypothesis is that the mean age is different between male and female respondents. The randomization test had a p-value of 0.0022 and therefore the results are significant and we can reject the null hypothesis that the mean age is the same between male and female respondents. WHen a  two-sample t-test was run, the p-value was again significant at 0.002574, revealing that both the randiomization test and two-sample t-test arrived at similar conclusions. We reject te null hypothesis that the mean age is the same between male and female respondents.The plot was created to visualize the null distribution, where the red lines represent the positive and negative test statistic, which was 7.413414.

## 3. Linear Regression Model 
```{r}
healthmx$hhsize_c<-healthmx$hhsize-mean(healthmx$hhsize,na.rm = T)
healthmx$hp_time_c<-healthmx$hp_time-mean(healthmx$hp_time,na.rm = T)
fitlr<-lm(hp_time_c~gender*hhsize_c,data=healthmx)
summary(fitlr)

#Regression
fitlr%>%ggplot(aes(hhsize_c,hp_time_c,color=gender))+geom_point()+geom_smooth(method="lm")+
  ggtitle("Regression of Household Size by Time to get to Healthcare Provider")

#Normality 
resids<-fitlr$residuals
fitvals<-fitlr$fitted.values
ggplot()+geom_histogram(aes(resids),bins=20)+ggtitle("Histogram of Residuals")
ggplot()+geom_qq(aes(sample=resids))+geom_qq()+ggtitle("QQ Plot")
shapiro.test(resids) 
#Linearity
ggplot()+geom_point(aes(resids,fitvals))+
ggtitle("Residuals vs. Fitted Values")
#Homoskedsaticity  
bptest(fitlr) #Reject the null hypothesis that the data is homoskedastic
#Redo the Regression Using Heteroskedasticity Robust Standard Errors 
coeftest(fitlr,vcov=vcovHC(fitlr))
#Variation Explained
summary(fitlr)$r.sq
```
For people with an average household size, male respondents have a higher average time to get to healthcare provider that is 37.379 minutes greater than the centered mean time for female respondents. When controlling for gender, those with an average household size take 2.535 minutes less to get to healthcare provider. The slope of houshold size on time to get to healthcare provider is 11.687 less for males as compared to females. 

The plot is the regression of centered household size and time to get to healthcare provider, colored by gender of the respondent.

The independent observations and random sampling assumption was met due to data collection design. To check normality, the Shapiro-Wilk normality test was run. Because the p-value of this test was very small, we reject the null hypothesis that the data is normally distributed and therefore the normality assumption was not met. This can also seen visually in the histogram, where the data does not appear to be normally distributed. To test for homoskedasticity, the Breuch-Pagan test was run and the p-value was significant, as a result we reject the null hypothesis that the data is homoskedastic and therefore the homoskedastic assumption was not met. Lastly, the linearity assumption appears to be met as seen by the linear relationship between the predictor and response variable.

The regression was recomputed with robust standard errors via coeftest(fitlr,vcov=vcovHC(fitlr)). After this was run, none of the coefficient variables (besides the intercept) were significant. Before this was run, the genderM variable coefficient was significant, but with robust standard errors it is no long signficant. When using the robust standard error, the standard errors increased in the genderM variable, going from 18.347 to 26.4348 with robust errors. The standard error interaction between genderM and household size also increased, where it was 11.611 previously and was 13.5863 with robust standard errors. There was little change in the coefficient estimates themselves between the linear regression and the linear regression with robust standard errors.

To determine the proportion of variation in the outcome that the model explains, the R squared value of the model was used. Based on the r-squared, the model explains 0.03385 proportion of variation, or 3.385%.


## 4.Linear Regression Model with Standard Errors

```{r}
healthmx$hhsize_c<-healthmx$hhsize-mean(healthmx$hhsize,na.rm = T)
healthmx$hp_time_c<-healthmx$hp_time-mean(healthmx$hp_time,na.rm = T)

set.seed(1)
samp_distn<-replicate(5000, {
boot_dat <- sample_frac(healthmx, replace=T) 
fit <- lm(hp_time_c~gender*hhsize_c,data=boot_dat)
coef(fit) 
})
samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)
```
The standard errors calculated with the boostrapped standard errors are very similar to the the standard errors calculated in the linear regression, but the errors are still smaller for bootstrapping. For the linear regression, the intercept SE was 3.2490, genderM SE was 26.4348, household size SE was 1.7971, and the interaction between household size and genderM SE was 13.5863. All of these values are larger than the SE calculated with the bootstrapping, where the Intercept SE was 3.20462, the genderM SE was 25.94308, household size SE was 1.790101, and the interaction between household size and genderM SE was 13.60298. This means that the smaller SE values from the bootstrapping have smaller p-values except for the interaction between household size and genderM. The standard errors are overall very similar.

##5. Logisitic Regression 
```{r}
dropna <- healthmx %>%drop_na(public_hp, community, hp_payment)
model <- glm(public_hp ~ community+ hp_payment,data=dropna,family=binomial(link="logit"))
summary(model)
coeftest(model)
exp(coef(model))

#Confusion Matrix
prob<-predict(model,type="response") 
pred<-ifelse(prob>.5,1,0)
table(prediction=pred, truth=dropna$public_hp)%>%addmargins

#Accuracy
(93+106)/234
#Sensitivity (TPR)
106/130
#Specificity (FPR)
93/104
#Precision (PPV)
106/117

#Density of Logodds 
dropna$logit<-predict(model,type='link')
dropna%>%ggplot()+geom_density(aes(logit,color=hcu,fill=hcu), alpha=.4)+theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("logit (log-odds)")+geom_rug(aes(logit,color=hcu))

#ROC
tdat<-dropna%>%mutate(prob=predict(model, type="response"), prediction=ifelse(prob>.5,1,0))
classify<-tdat%>%transmute(prob,prediction,truth=public_hp)
ROCplot<-ggplot(classify)+geom_roc(aes(d=truth,m=prob), n.cuts=0)
ROCplot
calc_auc(ROCplot)

#10-Fold CV
set.seed(1234)
k=10
data<-dropna[sample(nrow(dropna)),]
folds<-cut(seq(1:nrow(dropna)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$public_hp
fit<-glm(public_hp~ community+ hp_payment,data=train,family="binomial")
probs<-predict(fit,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
```
Controlling for community and paying for a part of healthcare services, paying for all of healthcare services has a significant negative impact on odds of going to a public healthcare provider. Controlling for community and paying for a part of healthcare services, the odds of going to a public healthcare provider is 0.00877388 times the odds of someone who pays for none of their healthcare services. Controlling for community and paying for all of healthcare services, paying for part of healthcare services does not have a significant negative impact on odds of going to a public healthcare provider. Controlling for payment for healthcare services, community does not significantly impact if someone goes to public healthcare provider.

To compute the accuracy, sensitivity, specificity, and recall a confusion matrix of the model was produced. The accuracy was computed to be 0.8504274, which is a good accuracy at which the model predicts public healthcare utilization. Sensitivity is the true positive rate which is the probability of predicting public healthcare utilization for people who actually use public healthcare services and was calculated to be 0.8153846. Specificity is the true negative rate and is the proability of a person who uses private services been predicted as using public healthcare services and was calculated to be 0.8942308. The precision is the proportion of people who were classified as public healthcare utilizers who actually are and was calculated to be 0.9059829.

Based on the density plot we can visualize the misclassified area, which is gray. To the right of 0, the gray area is the proportion of false positives and to the left of 0, the gray area is the proportion of false negatives of the model prediciting public healthcare utilization

The ROC plot was generated and a AUC was calculated. THe ROC curve alllows for visualization of the trade-off between sensitivity and specificity. The AUC was calculated to be 0.8884246, which is a good AUC and means that payment level and community are a good predictor of healthcare utilization type.

After performing a 10-fold CV, the average accuracy was calculcated to be 0.85,  the average sensitivity was calculated to be 0.8048194, the precision was calculcated to be 0.9015287, and the auc was calculated to be 0.8771041. These values, while slightly different, are very similar to the others calculated above. Overall, the model appears to be good at predicting healthcare utilization type.

##6. LASSO

```{r}
library(glmnet) 
set.seed(1234)
healthmx1<-healthmx%>%na.omit(public_hp,community,housholdstructure,hp_visit,transportation,
                  hp_difficulty,hp_payment,pay_difficulty,confidence,gender,hp_time,hhsize,age)
y<-as.matrix(healthmx1$public_hp) 
x<-model.matrix(public_hp~community+housholdstructure+hp_visit+transportation+
                  hp_difficulty+hp_payment+pay_difficulty+confidence+gender+hp_time+hhsize+age,data=healthmx1)
cv.lasso1<-cv.glmnet(x,y,family="binomial")
lasso1<-glmnet(x,y,family="binomial",lambda=cv.lasso1$lambda.1se)
coef(lasso1)

#CV Lasso
healthmx_1<-healthmx1%>%mutate(Caminando=ifelse(transportation=="Caminando",1,0))%>%
mutate(parte=ifelse(hp_payment=="Una parte",1,0))%>%select(public_hp, Caminando,parte)

set.seed(1234)
k=10
data <- healthmx_1 %>% sample_frac 
folds <- ntile(1:nrow(data),n=10) 
diags<-NULL
for(i in 1:k){
train <- data[folds!=i,] 
test <- data[folds==i,] 
truth <- test$public_hp 
fit <- glm(public_hp~Caminando+parte,data=train, family="binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
diags%>%summarize_all(mean)
```
The 10-fold CV was performed using the LASSO and revealed a lower accuracy at 0.8 as compared to the accuracy from the logisitic regression of 0.85. This means that the LASSO model has lower accuracy than the logisitc regression model when predicting public healthcare provider utilization. THe 10-fold CV using the LASSO had an AUC of 0.8551299 whereas the AUC from the logistic regression was 0.8771041. This means that the logistic regression model is better at predicting than the model produced from LASSO.